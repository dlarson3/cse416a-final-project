{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IGNORE THIS!!!!! IT'S JUST PREPROCESSING OF DATA \n",
    "#I just want to keep it for our submission\n",
    "\n",
    "\n",
    "#from dblplib import parse, parse_file\n",
    "import dblplib\n",
    "# from dblplib import parse_file\n",
    "from lxml import etree\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import ujson\n",
    "import codecs\n",
    "import re\n",
    "from github_com.kennethreitz import requests\n",
    "assert requests.get('https://github.com/IsaacChanghau/DBLPParser').status_code == 200\n",
    "\n",
    "\n",
    "# all of the element types in dblp\n",
    "all_elements = {\"article\", \"inproceedings\", \"proceedings\", \"book\", \"incollection\", \"phdthesis\", \"mastersthesis\", \"www\"}\n",
    "# all of the feature types in dblp\n",
    "all_features = {\"address\", \"author\", \"booktitle\", \"cdrom\", \"chapter\", \"cite\", \"crossref\", \"editor\", \"ee\", \"isbn\",\n",
    "                \"journal\", \"month\", \"note\", \"number\", \"pages\", \"publisher\", \"school\", \"series\", \"title\", \"url\",\n",
    "                \"volume\", \"year\"}\n",
    "\n",
    "\n",
    "def log_msg(message):\n",
    "    \"\"\"Produce a log with current time\"\"\"\n",
    "    print(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), message)\n",
    "\n",
    "\n",
    "def context_iter(dblp_path):\n",
    "    \"\"\"Create a dblp data iterator of (event, element) pairs for processing\"\"\"\n",
    "    return etree.iterparse(source=dblp_path, dtd_validation=True, load_dtd=True)  # required dtd\n",
    "\n",
    "\n",
    "def clear_element(element):\n",
    "    \"\"\"Free up memory for temporary element tree after processing the element\"\"\"\n",
    "    element.clear()\n",
    "    while element.getprevious() is not None:\n",
    "        del element.getparent()[0]\n",
    "\n",
    "\n",
    "def count_pages(pages):\n",
    "    \"\"\"Borrowed from: https://github.com/billjh/dblp-iter-parser/blob/master/iter_parser.py\n",
    "    Parse pages string and count number of pages. There might be multiple pages separated by commas.\n",
    "    VALID FORMATS:\n",
    "        51         -> Single number\n",
    "        23-43      -> Range by two numbers\n",
    "    NON-DIGITS ARE ALLOWED BUT IGNORED:\n",
    "        AG83-AG120\n",
    "        90210H     -> Containing alphabets\n",
    "        8e:1-8e:4\n",
    "        11:12-21   -> Containing colons\n",
    "        P1.35      -> Containing dots\n",
    "        S2/109     -> Containing slashes\n",
    "        2-3&4      -> Containing ampersands and more...\n",
    "    INVALID FORMATS:\n",
    "        I-XXI      -> Roman numerals are not recognized\n",
    "        0-         -> Incomplete range\n",
    "        91A-91A-3  -> More than one dash\n",
    "        f          -> No digits\n",
    "    ALGORITHM:\n",
    "        1) Split the string by comma evaluated each part with (2).\n",
    "        2) Split the part to subparts by dash. If more than two subparts, evaluate to zero. If have two subparts,\n",
    "           evaluate by (3). If have one subpart, evaluate by (4).\n",
    "        3) For both subparts, convert to number by (4). If not successful in either subpart, return zero. Subtract first\n",
    "           to second, if negative, return zero; else return (second - first + 1) as page count.\n",
    "        4) Search for number consist of digits. Only take the last one (P17.23 -> 23). Return page count as 1 for (2)\n",
    "           if find; 0 for (2) if not find. Return the number for (3) if find; -1 for (3) if not find.\n",
    "    \"\"\"\n",
    "    cnt = 0\n",
    "    for part in re.compile(r\",\").split(pages):\n",
    "        subparts = re.compile(r\"-\").split(part)\n",
    "        if len(subparts) > 2:\n",
    "            continue\n",
    "        else:\n",
    "            try:\n",
    "                re_digits = re.compile(r\"[\\d]+\")\n",
    "                subparts = [int(re_digits.findall(sub)[-1]) for sub in subparts]\n",
    "            except IndexError:\n",
    "                continue\n",
    "            cnt += 1 if len(subparts) == 1 else subparts[1] - subparts[0] + 1\n",
    "    return \"\" if cnt == 0 else str(cnt)\n",
    "\n",
    "\n",
    "def extract_feature(elem, features, include_key=False):\n",
    "    \"\"\"Extract the value of each feature\"\"\"\n",
    "    if include_key:\n",
    "        attribs = {'key': [elem.attrib['key']]}\n",
    "    else:\n",
    "        attribs = {}\n",
    "    for feature in features:\n",
    "        attribs[feature] = []\n",
    "    for sub in elem:\n",
    "        if sub.tag not in features:\n",
    "            continue\n",
    "        if sub.tag == 'title':\n",
    "            text = re.sub(\"<.*?>\", \"\", etree.tostring(sub).decode('utf-8')) if sub.text is None else sub.text\n",
    "        elif sub.tag == 'pages':\n",
    "            text = count_pages(sub.text)\n",
    "        else:\n",
    "            text = sub.text\n",
    "        if text is not None and len(text) > 0:\n",
    "            attribs[sub.tag] = attribs.get(sub.tag) + [text]\n",
    "    return attribs\n",
    "\n",
    "\n",
    "def parse_all(dblp_path, save_path, include_key=False):\n",
    "    log_msg(\"PROCESS: Start parsing...\")\n",
    "    f = open(save_path, 'w', encoding='utf8')\n",
    "    for _, elem in context_iter(dblp_path):\n",
    "        if elem.tag in all_elements:\n",
    "            attrib_values = extract_feature(elem, all_features, include_key)\n",
    "            f.write(str(attrib_values) + '\\n')\n",
    "        clear_element(elem)\n",
    "    f.close()\n",
    "    log_msg(\"FINISHED...\")  # load the saved results line by line using json\n",
    "\n",
    "\n",
    "def parse_entity(dblp_path, save_path, type_name, features=None, save_to_csv=False, include_key=False):\n",
    "    \"\"\"Parse specific elements according to the given type name and features\"\"\"\n",
    "    log_msg(\"PROCESS: Start parsing for {}...\".format(str(type_name)))\n",
    "    assert features is not None, \"features must be assigned before parsing the dblp dataset\"\n",
    "    results = []\n",
    "    attrib_count, full_entity, part_entity = {}, 0, 0\n",
    "    for _, elem in context_iter(dblp_path):\n",
    "        if elem.tag in type_name:\n",
    "            attrib_values = extract_feature(elem, features, include_key)  # extract required features\n",
    "            results.append(attrib_values)  # add record to results array\n",
    "            for key, value in attrib_values.items():\n",
    "                attrib_count[key] = attrib_count.get(key, 0) + len(value)\n",
    "            cnt = sum([1 if len(x) > 0 else 0 for x in list(attrib_values.values())])\n",
    "            if cnt == len(features):\n",
    "                full_entity += 1\n",
    "            else:\n",
    "                part_entity += 1\n",
    "        elif elem.tag not in all_elements:\n",
    "            continue\n",
    "        clear_element(elem)\n",
    "    if save_to_csv:\n",
    "        f = open(save_path, 'w', newline='', encoding='utf8')\n",
    "        writer = csv.writer(f, delimiter=',')\n",
    "        writer.writerow(features)  # write title\n",
    "        for record in results:\n",
    "            # some features contain multiple values (e.g.: author), concatenate with `::`\n",
    "            row = ['::'.join(v) for v in list(record.values())]\n",
    "            writer.writerow(row)\n",
    "        f.close()\n",
    "    else:  # default save to json file\n",
    "        with codecs.open(save_path, mode='w', encoding='utf8', errors='ignore') as f:\n",
    "            ujson.dump(results, f)\n",
    "    return full_entity, part_entity, attrib_count\n",
    "\n",
    "\n",
    "def parse_author(dblp_path, save_path, save_to_csv=False):\n",
    "    type_name = ['article', 'book', 'incollection', 'inproceedings']\n",
    "    log_msg(\"PROCESS: Start parsing for {}...\".format(str(type_name)))\n",
    "    authors = set()\n",
    "    for _, elem in context_iter(dblp_path):\n",
    "        if elem.tag in type_name:\n",
    "            authors.update(a.text for a in elem.findall('author'))\n",
    "        elif elem.tag not in all_elements:\n",
    "            continue\n",
    "        clear_element(elem)\n",
    "    if save_to_csv:\n",
    "        f = open(save_path, 'w', newline='', encoding='utf8')\n",
    "        writer = csv.writer(f, delimiter=',')\n",
    "        writer.writerows([a] for a in sorted(authors))\n",
    "        f.close()\n",
    "    else:\n",
    "        with open(save_path, 'w', encoding='utf8') as f:\n",
    "            f.write('\\n'.join(sorted(authors)))\n",
    "    log_msg(\"FINISHED...\")\n",
    "\n",
    "\n",
    "def parse_article(dblp_path, save_path, save_to_csv=False, include_key=False):\n",
    "    type_name = ['article']\n",
    "    features = ['title', 'author', 'year', 'journal', 'pages']\n",
    "    info = parse_entity(dblp_path, save_path, type_name, features, save_to_csv=save_to_csv, include_key=include_key)\n",
    "    log_msg('Total articles found: {}, articles contain all features: {}, articles contain part of features: {}'\n",
    "            .format(info[0] + info[1], info[0], info[1]))\n",
    "    log_msg(\"Features information: {}\".format(str(info[2])))\n",
    "\n",
    "\n",
    "def parse_inproceedings(dblp_path, save_path, save_to_csv=False, include_key=False):\n",
    "    type_name = [\"inproceedings\"]\n",
    "    features = ['title', 'author', 'year', 'pages', 'booktitle']\n",
    "    info = parse_entity(dblp_path, save_path, type_name, features, save_to_csv=save_to_csv, include_key=include_key)\n",
    "    log_msg('Total inproceedings found: {}, inproceedings contain all features: {}, inproceedings contain part of '\n",
    "            'features: {}'.format(info[0] + info[1], info[0], info[1]))\n",
    "    log_msg(\"Features information: {}\".format(str(info[2])))\n",
    "\n",
    "\n",
    "def parse_proceedings(dblp_path, save_path, save_to_csv=False, include_key=False):\n",
    "    type_name = [\"proceedings\"]\n",
    "    features = ['title', 'editor', 'year', 'booktitle', 'series', 'publisher']\n",
    "    # Other features are 'volume','isbn' and 'url'.\n",
    "    info = parse_entity(dblp_path, save_path, type_name, features, save_to_csv=save_to_csv, include_key=include_key)\n",
    "    log_msg('Total proceedings found: {}, proceedings contain all features: {}, proceedings contain part of '\n",
    "            'features: {}'.format(info[0] + info[1], info[0], info[1]))\n",
    "    log_msg(\"Features information: {}\".format(str(info[2])))\n",
    "\n",
    "\n",
    "def parse_book(dblp_path, save_path, save_to_csv=False, include_key=False):\n",
    "    type_name = [\"book\"]\n",
    "    features = ['title', 'author', 'publisher', 'isbn', 'year', 'pages']\n",
    "    info = parse_entity(dblp_path, save_path, type_name, features, save_to_csv=save_to_csv, include_key=include_key)\n",
    "    log_msg('Total books found: {}, books contain all features: {}, books contain part of features: {}'\n",
    "            .format(info[0] + info[1], info[0], info[1]))\n",
    "    log_msg(\"Features information: {}\".format(str(info[2])))\n",
    "\n",
    "\n",
    "def parse_publications(dblp_path, save_path, save_to_csv=False, include_key=False):\n",
    "    type_name = ['article', 'book', 'incollection', 'inproceedings']\n",
    "    features = ['title', 'year', 'pages']\n",
    "    info = parse_entity(dblp_path, save_path, type_name, features, save_to_csv=save_to_csv, include_key=include_key)\n",
    "    log_msg('Total publications found: {}, publications contain all features: {}, publications contain part of '\n",
    "            'features: {}'.format(info[0] + info[1], info[0], info[1]))\n",
    "    log_msg(\"Features information: {}\".format(str(info[2])))\n",
    "\n",
    "\n",
    "dblp_path = '/Users/daniellelarson/cse416/final/dblp-2019-11-01.xml'\n",
    "save_path = '/Users/daniellelarson/cse416/final/article.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-01 23:46:11 LOG: Successfully loaded \"/Users/daniellelarson/cse416/final/dblp-2019-11-01.xml\".\n",
      "2019-12-01 23:46:11 PROCESS: Start parsing for ['article']...\n",
      "2019-12-01 23:49:00 Total articles found: 2137512, articles contain all features: 1815958, articles contain part of features: 321554\n",
      "2019-12-01 23:49:00 Features information: {'title': 2137512, 'author': 6264506, 'year': 2137509, 'journal': 2137285, 'pages': 1824941}\n",
      "CPU times: user 2min 43s, sys: 4.53 s, total: 2min 48s\n",
      "Wall time: 2min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "try:\n",
    "    context_iter(dblp_path)\n",
    "    log_msg(\"LOG: Successfully loaded \\\"{}\\\".\".format(dblp_path))\n",
    "except IOError:\n",
    "    log_msg(\"ERROR: Failed to load file \\\"{}\\\". Please check your XML and DTD files.\".format(dblp_path))\n",
    "    exit()\n",
    "parse_article(dblp_path, save_path, save_to_csv=False)  # default save as json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-02 00:04:08 LOG: Successfully loaded \"/Users/daniellelarson/cse416/final/dblp-2019-11-01.xml\".\n",
      "2019-12-02 00:04:08 PROCESS: Start parsing for ['article']...\n",
      "2019-12-02 00:07:00 Total articles found: 2137512, articles contain all features: 1815958, articles contain part of features: 321554\n",
      "2019-12-02 00:07:00 Features information: {'title': 2137512, 'author': 6264506, 'year': 2137509, 'journal': 2137285, 'pages': 1824941}\n",
      "CPU times: user 2min 46s, sys: 4.21 s, total: 2min 51s\n",
      "Wall time: 2min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "save_path = '/Users/daniellelarson/cse416/final/article.csv'\n",
    "try:\n",
    "    context_iter(dblp_path)\n",
    "    log_msg(\"LOG: Successfully loaded \\\"{}\\\".\".format(dblp_path))\n",
    "except IOError:\n",
    "    log_msg(\"ERROR: Failed to load file \\\"{}\\\". Please check your XML and DTD files.\".format(dblp_path))\n",
    "    exit()\n",
    "parse_article(dblp_path, save_path, save_to_csv=True)  # default save as json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.4 s, sys: 831 ms, total: 13.3 s\n",
      "Wall time: 13.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#RUN THIS: START HERE TO CREATE GRAPH\n",
    "\n",
    "#load in from csv (obvs change path)\n",
    " \n",
    "import csv\n",
    "jordan = 'edges_no_title.csv'\n",
    "danielle = '/Users/daniellelarson/cse416/final/edges_no_title.csv'\n",
    "with open(jordan, 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    edges = list(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 23s, sys: 2.22 s, total: 1min 25s\n",
      "Wall time: 1min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#IGNORE THIS \n",
    "#It was just to create the original file with the gendered items\n",
    " \n",
    "\n",
    "from genderize import Genderize\n",
    "import gender_guesser.detector as gender\n",
    "d = gender.Detector()\n",
    "genders = []\n",
    "count = 0\n",
    "for e in edges:\n",
    "    name1 = e[0]\n",
    "    name2 = e[1]\n",
    "    first1 = name1.split(' ', 1)[0]\n",
    "    first2 = name2.split(' ', 1)[0]\n",
    "    #print(first1)\n",
    "    #print(first2)\n",
    "    g1 = d.get_gender(first1)\n",
    "    g2 = d.get_gender(first2)\n",
    "    #print(g1)\n",
    "    #print(g2)\n",
    "    Dict1 = {\"name\": name1, \"gender\": g1} \n",
    "    Dict2 = {\"name\": name2, \"gender\": g2} \n",
    "    genders.append([Dict1,Dict2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IGNORE THIS \n",
    "import json\n",
    "jordan = 'no_title_edges_with_gender'\n",
    "danielle = '/Users/daniellelarson/cse416/final/no_title_edges_with_gender'\n",
    "with open(jordan, 'w') as fout:\n",
    "    json.dump(genders, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IGNORE THIS \n",
    "import csv\n",
    "\n",
    "with open(\"no_title_edges_with_gender.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(genders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.4 s, sys: 5.56 s, total: 28 s\n",
      "Wall time: 30.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#RUN THIS\n",
    "\n",
    "#create base graph\n",
    "\n",
    "import networkx as nx\n",
    "G=nx.Graph()\n",
    "G.add_edges_from(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 41s, sys: 1.16 s, total: 1min 42s\n",
      "Wall time: 1min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#RUN THIS\n",
    "\n",
    "#label edges\n",
    "\n",
    "#G[1][3]['color']='blue'\n",
    "#assigning genders to edges\n",
    "for e in edges:\n",
    "    name1 = e[0]\n",
    "    name2 = e[1]\n",
    "    first1 = name1.split(' ', 1)[0]\n",
    "    first2 = name2.split(' ', 1)[0]\n",
    "    #print(first1)\n",
    "    #print(first2)\n",
    "    g1 = d.get_gender(first1)\n",
    "    g2 = d.get_gender(first2)\n",
    "    #print(g1)\n",
    "    #print(g2)\n",
    "#     if (str(g1) == \"andy\" or str(g1) == \"unknown\"):\n",
    "#         G.remove_node(name1)\n",
    "#     elif (str(g2) == \"andy\" or str(g2) == \"unknown\"):\n",
    "#         G.remove_node(name2)\n",
    "    if (g1 == g2) :\n",
    "        if(str(g1) == \"male\"):\n",
    "            G[name1][name2]['gender'] = 'blue'\n",
    "        else:\n",
    "            G[name1][name2]['gender'] = 'pink'\n",
    "    else:\n",
    "        G[name1][name2]['gender'] = 'purple'\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.52 s, sys: 49.3 ms, total: 8.57 s\n",
      "Wall time: 8.57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#RUN THIS\n",
    "\n",
    "#label nodes\n",
    "\n",
    "\n",
    "#G.node[1]['room'] = 714\n",
    "#assigning genders to nodes\n",
    "for n in G.nodes():\n",
    "    first = n.split(' ', 1)[0]\n",
    "    G.nodes[n]['gender'] = d.get_gender(first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "942342\n",
      "942342\n",
      "CPU times: user 1.86 s, sys: 5.09 ms, total: 1.86 s\n",
      "Wall time: 1.86 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#RUN THIS\n",
    "\n",
    "\n",
    "\n",
    "#removing unknown or andy\n",
    "nodes = []\n",
    "nodes = G.nodes()\n",
    "delNodes = []\n",
    "print(G.number_of_nodes())\n",
    "for n in nodes:\n",
    "    if str(G.nodes[n]['gender']) == \"andy\" :\n",
    "        delNodes.append(n)\n",
    "    elif str(G.nodes[n]['gender']) == \"unknown\":\n",
    "        delNodes.append(n)\n",
    "        \n",
    "G.remove_nodes_from(delNodes)\n",
    "print(G.number_of_nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in nodes:\n",
    "    if str(G.nodes[n]['gender']) == \"mostly_male\" :\n",
    "        G.nodes[n]['gender'] = \"male\"\n",
    "    elif str(G.nodes[n]['gender']) == \"mostly_female\":\n",
    "        G.nodes[n]['gender'] = \"female\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## output only male and females to file (removed unknowns and androgenous names)\n",
    "## ex:\n",
    "## Frank, male\n",
    "## Anna, female\n",
    "import random\n",
    "import pandas\n",
    "\n",
    "mf_list = []\n",
    "mf_list2 = []\n",
    "bad_chars = [\"(\", \")\", \"'\"] \n",
    "random_sample = random.sample(G.nodes(),k=471171) #half of the dataset (G.number_of_nodes()/2)\n",
    "for n in random_sample:\n",
    "    mf_list.append(n)\n",
    "    mf_list2.append(G.nodes[n]['gender'])\n",
    "\n",
    "pd = pandas.DataFrame(mf_list,mf_list2)\n",
    "pd.to_csv(\"only_authors_and_genders.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
